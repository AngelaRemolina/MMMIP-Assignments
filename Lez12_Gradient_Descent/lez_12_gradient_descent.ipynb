{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7W8NvCtBd5f+WP42XNDGw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pLAA3Lj3GLtp"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","source":["Define the problem parameters\n"],"metadata":{"id":"Be43ir2EHcSE"}},{"cell_type":"code","source":["A = np.array([[1, 3], [3, 1]]) # low dimensions to plot it, you can test larger sizes\n","b = np.array([1.1, 0.2])"],"metadata":{"id":"8Sh4rbXeHh_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function to be minimized is $\\frac{1}{2}\\|Ax-b\\|_2^2$"],"metadata":{"id":"qjOQIg-ZHpTc"}},{"cell_type":"code","source":["f = lambda x: 0.5*np.sum((A @ x - b) ** 2)\n","\n","# derivative of f from matrix calculus\n","df = lambda x: A.T @ (A @ x - b)\n"],"metadata":{"id":"-IZ3t2qiHoOY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the function"],"metadata":{"id":"wjmesSGYH4YK"}},{"cell_type":"code","source":["# this function has been prepared only for the visualization sake, no need to go through this but it renders some nice\n","# graphics :)\n","F = lambda r1, r2: (r1 * A[0, 0] + r2 * A[0, 1] - b[0]) ** 2 + (r1 * A[1, 0] + r2 * A[1, 1] - b[1]) ** 2\n","xx, yy = np.meshgrid(np.arange(-10, 10), np.arange(-10, 10))\n","\n","fig = plt.figure()\n","ax = plt.axes(projection='3d')\n","ax.plot_surface(xx, yy, F(xx, yy), edgecolor=[0, 0, 1], alpha=0.5, facecolor=[0, 1, 1])"],"metadata":{"id":"NXdKkXicH8-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parameters of gradient descent"],"metadata":{"id":"ec2jaZBWIHDf"}},{"cell_type":"code","source":["TOL_GRAD_NORM = 1e-5\n","MAX_ITER = 100\n","TOL_DIST = 1e-4\n","alpha = 0.05 #  step size.. play with this, the system might get really unstable"],"metadata":{"id":"qd8R8ya6IIQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialization: test different inizializations, the function is convex, you always converge to the same solution"],"metadata":{"id":"ZZXmb-rbISJb"}},{"cell_type":"code","source":["x = np.array([-5, 10])\n","\n","grad_norm = 1e10\n","distanceX = 1e10\n","\n","# initialize the list with all the estimates\n","all_x = [x]\n"],"metadata":{"id":"Uelx1xRoIXh_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Main loop"],"metadata":{"id":"8dlJ-5peIbf6"}},{"cell_type":"code","source":["cnt = 0\n","\n","while grad_norm > TOL_GRAD_NORM and cnt < MAX_ITER and distanceX > TOL_DIST:\n","    cnt = cnt + 1\n","    # gradient descent step\n","    # x =\n","\n","    # compute the norm of the gradient for the stopping criteria\n","    # grad_norm =\n","\n","    # compute the distance between two consecutive iterates for the stopping criteria\n","    # distanceX =\n","\n","    # store the estimate\n","    # all_x"],"metadata":{"id":"1_V3FeuvIasS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot all the estimates"],"metadata":{"id":"9wG7LahlIjoJ"}},{"cell_type":"code","source":["# plot the new estimate\n","xxplot = [x[0] for x in all_x]\n","yyplot = [x[1] for x in all_x]\n","zzplot = F(np.array(xxplot), np.array(yyplot))\n","\n","fig = plt.figure(figsize=(10,10))\n","ax = plt.axes(projection='3d')\n","ax.plot_surface(xx, yy, F(xx, yy), edgecolor=[0, 0, 1], alpha=0.5, facecolor=[0, 1, 1])\n","ax.plot3D(xxplot, yyplot, zzplot, 'r-o')"],"metadata":{"id":"n0JPj1VPIkkR"},"execution_count":null,"outputs":[]}]}